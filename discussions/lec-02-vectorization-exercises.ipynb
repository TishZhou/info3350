{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c17f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ccd47f",
   "metadata": {},
   "source": [
    "# Discussion Section Exercise: Week 2\n",
    "Since there's no homework assigned yet, this notebook is meant to gently introduce some of the Python packages we've been discussing in lecture.\n",
    "\n",
    "The goal of this exercise isn't to get a \"right answer\", but instead to introduce to `sklearn` and its documentation. I've provided some suggested code at the bottom of this notebook. You should only refer to these suggestions after you have put in some time working independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791955f",
   "metadata": {},
   "source": [
    "## Loading some toy data\n",
    "\n",
    "Below are some sentences that we'll be working with in following cells. They are all <I>incipits</I> (you pronounce the \"c\" like a \"k\"), which are the first sentence(s) of a written work. I found them [here](https://www.penguin.co.uk/discover/articles/best-first-lines-in-books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d082c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "incipits = [\n",
    "    \"Mother died today. Or maybe, yesterday; I can't be sure.\", # The Outsider by Camus\n",
    "    \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\",  # Metamorphosis by Franz Kafka\n",
    "    \"124 was spiteful. Full of Baby's venom.\", # Beloved by Toni Morrison\n",
    "    \"The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move.\", # The Restaurant at the End of the Universe by Douglas Adams\n",
    "    \"It was a bright cold day in April, and the clocks were striking thirteen.\", # 1984 by George Orwell\n",
    "    \"I write this sitting in the kitchen sink.\" # I Capture the Castle by Dodie Smith\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574ca6e",
   "metadata": {},
   "source": [
    "## Counting tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56ff6d",
   "metadata": {},
   "source": [
    "First, let's vectorize these incipits using a `CountVectorizer`, which we imported in the first cell. Try first without passing any arguments to `CountVectorizer` when initializing it. Print the vectorized sequences after you've run your vectorizer on `incipits` as a `numpy array` (not a CSR sparse matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e91add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b89fc",
   "metadata": {},
   "source": [
    "Cool. What's the shape of this array? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86c93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b98251",
   "metadata": {},
   "source": [
    "Now inspect the `CountVectorizer`'s vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270deba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2948b",
   "metadata": {},
   "source": [
    "Using the vocabulary, print the counts of the word \"the\" for all sequences in `incipits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fd650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82885fd6",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Now, let's convert the count vectors you computed to TF-IDF vectors using a `TFIDFTransformer`. Print out an `array` of vectors, as well as its shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11e904d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66d324",
   "metadata": {},
   "source": [
    "Now, let's try using the `TfidfVectorizer`, which is the same as running `CountVectorizer` followed by `TfidfTransformer`. Use `TfidfVectorizer` to vectorize `incipits`.\n",
    "\n",
    "This time, though, when initializing the `TfidfVectorizer`, use `sklearn`'s built-in English stopword list. In addition, limit your vocabulary to the top 15 most frequent words across all sequences. Check the docs [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to find out how to do this.\n",
    "\n",
    "Again, print the vectors as an array, print its shape, and print the `TfidfVectorizer`'s vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f31b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d104b",
   "metadata": {},
   "source": [
    "## Suggested code\n",
    "\n",
    "Please don't look at the below until doing the above yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1ca73",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 3, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "count_vectorizer = CountVectorizer()\n",
    "incipit_vectors = count_vectorizer.fit_transform(incipits).toarray() # toarray casts CSR sparse matrix to array\n",
    "incipit_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e2e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 68)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "incipit_vectors.shape # Gets the shape: (num_sequences, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87100667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mother': 39,\n",
       " 'died': 18,\n",
       " 'today': 57,\n",
       " 'or': 43,\n",
       " 'maybe': 37,\n",
       " 'yesterday': 67,\n",
       " 'can': 13,\n",
       " 'be': 8,\n",
       " 'sure': 53,\n",
       " 'as': 4,\n",
       " 'gregor': 25,\n",
       " 'samsa': 46,\n",
       " 'awoke': 5,\n",
       " 'one': 42,\n",
       " 'morning': 38,\n",
       " 'from': 22,\n",
       " 'uneasy': 59,\n",
       " 'dreams': 19,\n",
       " 'he': 27,\n",
       " 'found': 21,\n",
       " 'himself': 28,\n",
       " 'transformed': 58,\n",
       " 'in': 30,\n",
       " 'his': 29,\n",
       " 'bed': 9,\n",
       " 'into': 32,\n",
       " 'gigantic': 24,\n",
       " 'insect': 31,\n",
       " '124': 0,\n",
       " 'was': 63,\n",
       " 'spiteful': 50,\n",
       " 'full': 23,\n",
       " 'of': 41,\n",
       " 'baby': 6,\n",
       " 'venom': 61,\n",
       " 'the': 54,\n",
       " 'story': 51,\n",
       " 'so': 49,\n",
       " 'far': 20,\n",
       " 'beginning': 11,\n",
       " 'universe': 60,\n",
       " 'created': 16,\n",
       " 'this': 56,\n",
       " 'has': 26,\n",
       " 'made': 36,\n",
       " 'lot': 35,\n",
       " 'people': 44,\n",
       " 'very': 62,\n",
       " 'angry': 2,\n",
       " 'and': 1,\n",
       " 'been': 10,\n",
       " 'widely': 65,\n",
       " 'regarded': 45,\n",
       " 'bad': 7,\n",
       " 'move': 40,\n",
       " 'it': 33,\n",
       " 'bright': 12,\n",
       " 'cold': 15,\n",
       " 'day': 17,\n",
       " 'april': 3,\n",
       " 'clocks': 14,\n",
       " 'were': 64,\n",
       " 'striking': 52,\n",
       " 'thirteen': 55,\n",
       " 'write': 66,\n",
       " 'sitting': 48,\n",
       " 'kitchen': 34,\n",
       " 'sink': 47}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3\n",
    "count_vectorizer.vocabulary_ # Note the underscore. It's a dict of {token: index}, where the index\n",
    "                             # corresponds to the location of that token's counts in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c409c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 3, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4\n",
    "incipit_vectors[:, count_vectorizer.vocabulary_[\"the\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753ee9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectors:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.33333333 0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333]\n",
      " [0.         0.         0.         0.         0.19314847 0.2355428\n",
      "  0.         0.         0.         0.2355428  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2355428  0.         0.2355428  0.2355428  0.\n",
      "  0.2355428  0.2355428  0.         0.2355428  0.2355428  0.2355428\n",
      "  0.13973792 0.2355428  0.2355428  0.         0.         0.\n",
      "  0.         0.         0.2355428  0.         0.         0.\n",
      "  0.2355428  0.         0.         0.         0.2355428  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2355428  0.2355428\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.40318254 0.         0.         0.         0.         0.\n",
      "  0.40318254 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40318254\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33061545\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.40318254 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.40318254 0.         0.27912828 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.16454804 0.20066484 0.         0.16454804 0.\n",
      "  0.         0.20066484 0.         0.         0.20066484 0.20066484\n",
      "  0.         0.         0.         0.         0.20066484 0.\n",
      "  0.         0.         0.20066484 0.         0.         0.\n",
      "  0.         0.         0.20066484 0.         0.         0.\n",
      "  0.11904625 0.         0.         0.         0.         0.20066484\n",
      "  0.20066484 0.         0.         0.         0.20066484 0.16454804\n",
      "  0.         0.         0.20066484 0.20066484 0.         0.\n",
      "  0.         0.20066484 0.         0.20066484 0.         0.\n",
      "  0.41676828 0.         0.16454804 0.         0.         0.\n",
      "  0.20066484 0.         0.20066484 0.13892276 0.         0.20066484\n",
      "  0.         0.        ]\n",
      " [0.         0.2474352  0.         0.30174497 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30174497 0.         0.30174497 0.30174497 0.         0.30174497\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17901297 0.         0.         0.30174497 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30174497 0.\n",
      "  0.20890179 0.30174497 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.20890179 0.30174497 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25288205 0.         0.         0.         0.42625899 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.42625899\n",
      "  0.42625899 0.         0.         0.         0.         0.\n",
      "  0.2951044  0.         0.34953847 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.42625899 0.        ]]\n",
      "TF-IDF Array Shape: (6, 68)\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "tfidf_transformer = TfidfTransformer() \n",
    "tfidf_incipit = tfidf_transformer.fit_transform(incipit_vectors).toarray() \n",
    "\n",
    "print(f\"TF-IDF Vectors:\\n{tfidf_incipit}\\nTF-IDF Array Shape: {tfidf_incipit.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6b89496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectors with English stopwords and |V| <= 15:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.        ]\n",
      " [0.         0.         0.         0.57735027 0.         0.\n",
      "  0.57735027 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.57735027]\n",
      " [0.70710678 0.         0.         0.         0.70710678 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.5        0.         0.         0.         0.5\n",
      "  0.         0.5        0.         0.         0.         0.5\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.4472136  0.         0.         0.\n",
      "  0.         0.         0.4472136  0.4472136  0.4472136  0.\n",
      "  0.4472136  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "\n",
      "Shape: (6, 15)\n",
      "\n",
      "Vocab:\n",
      " {'died': np.int64(13), 'awoke': np.int64(3), 'dreams': np.int64(14), 'bed': np.int64(6), '124': np.int64(0), 'baby': np.int64(4), 'beginning': np.int64(7), 'created': np.int64(11), 'angry': np.int64(1), 'bad': np.int64(5), 'bright': np.int64(8), 'cold': np.int64(10), 'day': np.int64(12), 'april': np.int64(2), 'clocks': np.int64(9)}\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=15,\n",
    ")\n",
    "\n",
    "tfidf_incipit_small_vocab = tfidf_vectorizer.fit_transform(incipits).toarray()\n",
    "\n",
    "print(f\"TF-IDF vectors with English stopwords and |V| <= 15:\\n{tfidf_incipit_small_vocab}\")\n",
    "print(f\"\\nShape:\", tfidf_incipit_small_vocab.shape)\n",
    "print(\"\\nVocab:\\n\", tfidf_vectorizer.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
