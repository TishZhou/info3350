{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7e2426",
   "metadata": {},
   "source": [
    "# Arrays with Numpy and PyTorch\n",
    "\n",
    "This course has so far been (mostly) content to use sparse representations of text data. Next week, that is going to change--we will start looking at ways to transform text into dense vectors.\n",
    "\n",
    "Today's lecture is going to look at Numpy and PyTorch's arrays (or, in PyTorch language, \"tensors\"). This is getting ahead of ourselves a bit, since we haven't introduced neural methods yet, but it will be helpful to see some of the \"building blocks\" in code before we introduce the \"ideas\" behind neural models themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "526e6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223178f",
   "metadata": {},
   "source": [
    "## Dot product and matmul in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc906b7",
   "metadata": {},
   "source": [
    "Let's look at one-dimensional array `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cde4352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f2539",
   "metadata": {},
   "source": [
    "and one-dimensional array `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8240b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.tile(np.array([0, 1]), 5)\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b8198",
   "metadata": {},
   "source": [
    "If we take the dot product of these two arrays, what are we doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11094f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dot_b = np.dot(a, b)\n",
    "a_dot_b.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8aaf4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 3, 0, 5, 0, 7, 0, 9]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot(a, b):\n",
    "    assert a.shape == b.shape and len(a.shape) == 1\n",
    "    total = 0\n",
    "    element_prod = []\n",
    "    for i in range(a.shape[0]):\n",
    "        prod = (a[i]*b[i]).item()\n",
    "        element_prod.append(prod)\n",
    "        total += prod\n",
    "    return total, element_prod\n",
    "\n",
    "a_dot_b_loop, elements = dot(a, b)\n",
    "\n",
    "assert a_dot_b == a_dot_b_loop\n",
    "\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9ad703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249a480",
   "metadata": {},
   "source": [
    "The dot product of `a` and `b` is just the sum of the product of each element in a with the corresponding element in `b`. \n",
    "\n",
    "When we put this in a loop, we can see that the dot product of these two arrays is the same as looping over the corresponding elements of each vector, multiplying them together, and accumulating. When we look at `element_prod`, we can see these elementwiise products before summing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55be5c8",
   "metadata": {},
   "source": [
    "What about two-dimensional arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd29ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.repeat(a[None, :], 3, axis=0)\n",
    "print(A.shape)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a05b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.repeat(b[None, :], 3, axis=0)\n",
    "print(B.shape)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae23dd",
   "metadata": {},
   "source": [
    "Here, we've repeated vector `a` and `b` three times on the row (0th) axis to build `A` and `B`, respectively.\n",
    "\n",
    "Can we multiply these matrices together in the same way as we did with dot product above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5d29a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    np.dot(A, B)\n",
    "except ValueError: \n",
    "    print(\"Oops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91532e43",
   "metadata": {},
   "source": [
    "Why doesn't this work?\n",
    "\n",
    "Numpy's `dot` product turns into matrix multiplication or \"matmul\" when dealing with two 2-d arrays. \n",
    "\n",
    "Matrix multiplicaiton is the dot product of the rows in A with the columns in B. This means that, for matrix multiplication to work, the number of columns in A has to be the same as number of rows in B. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f14cf8",
   "metadata": {},
   "source": [
    "![](images/matrix-multiplication.gif)\n",
    "\n",
    "[(Image source)](https://notesbylex.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6d2f6",
   "metadata": {},
   "source": [
    "Knowing this, what can we do to make these two matrices \"compatible\" with one another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f5e699f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a3ae5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38620aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape[1] == B.T.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdc2e6",
   "metadata": {},
   "source": [
    "As we can see looking at the shape, `.T` transposes the axes of B. Now,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "236a658b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 25, 25],\n",
       "       [25, 25, 25],\n",
       "       [25, 25, 25]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AB = A @ B.T\n",
    "AB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f32fc",
   "metadata": {},
   "source": [
    "The `@` symbol is just matmul, the same as if we had called `np.numpy` on two 2-d arrays or used `np.matmul`. The new matrix `AB` is made up of 3 rows and 3 columns, which correspond to the dot products of the rows of A with the columns of the transpose of B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2bc36dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AB.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a3121",
   "metadata": {},
   "source": [
    "But!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d1be519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.T @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae74ea",
   "metadata": {},
   "source": [
    "Matrix multiplication is not commutative. `A @ B.T` gives us a 3x3 array, but `B.T @ A` is 10x10. If we do the latter, we are multiplying the rows of the transpose of B with the columns in A. That is not the same as what we were doing before!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604bba6",
   "metadata": {},
   "source": [
    "But why do we care about this?\n",
    "\n",
    "\n",
    "Let's implement matmul using loops and our `dot` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "daf53827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul2d(a, b):\n",
    "    assert len(a.shape) == len(b.shape) == 2\n",
    "    a_n, a_d = a.shape\n",
    "    b_n, b_d = b.shape\n",
    "    assert a_d == b_n\n",
    "\n",
    "    mat = np.zeros((a_n, b_d))\n",
    "    for i in range(a_n):\n",
    "        for j in range(b_d):\n",
    "            ai_bj, _ = dot(a[i, :], b[:, j])\n",
    "            mat[i, j] = ai_bj\n",
    "    \n",
    "    return mat\n",
    "\n",
    "assert np.allclose(matmul2d(A, B.T), A @ B.T)\n",
    "assert np.allclose(matmul2d(B.T, A), B.T @ A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc1a69",
   "metadata": {},
   "source": [
    "This is a pretty complex operation (there's a whole [article on the time complexity of matrix multiplication on Wikipedia](https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication), if you're interested). Our simple, (two in `matmul`, one in `dot`).\n",
    "\n",
    "In practical terms, Numpy matmul is very fast compared to Python loops, since it is both optimized and implemented in C. So if you want to get, for example, the pairwise cosine similarity of a bunch of vectors, you can do it like this quickly and without explicitly looping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3df51729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28, 20,  4, 15, 24,  1, 14,  9, 18, 12],\n",
       "       [24,  2, 11,  5,  5, 19, 23,  0, 14,  4],\n",
       "       [24, 21, 18, 25,  4, 22, 17, 28,  5,  3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_counts = np.random.randint(30, size=(3, 10))\n",
    "fake_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8bd9887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70562835, 0.72552703],\n",
       "       [0.70562835, 1.        , 0.71975844],\n",
       "       [0.72552703, 0.71975844, 1.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_fake_counts = np.sqrt(\n",
    "    np.sum(fake_counts**2, axis=1),\n",
    ")[:, None] # What does this do?\n",
    "\n",
    "print(l2_fake_counts.shape)\n",
    "\n",
    "normed_fake_counts = fake_counts / l2_fake_counts\n",
    "\n",
    "pairwise_nfc = normed_fake_counts @ normed_fake_counts.T\n",
    "\n",
    "pairwise_nfc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a2fae44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52.41183073],\n",
       "       [43.0464865 ],\n",
       "       [59.77457654]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_fake_counts = np.sqrt(\n",
    "    np.sum(fake_counts**2, axis=1),\n",
    ")[:, None]\n",
    "l2_fake_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbade6",
   "metadata": {},
   "source": [
    "We'll talk more about how matrix multiplication fits into neural networks later on, but consider these examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb555ed",
   "metadata": {},
   "source": [
    "- Permuting a matrix using another matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3bb7adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.arange(15).reshape((5, 3))\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0365194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17cea47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  1],\n",
       "       [ 3,  5,  4],\n",
       "       [ 6,  8,  7],\n",
       "       [ 9, 11, 10],\n",
       "       [12, 14, 13]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C @ P # Swap 2nd and 3rd columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "899bfd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  3,  6,  9, 12],\n",
       "       [ 1,  4,  7, 10, 13],\n",
       "       [ 2,  5,  8, 11, 14]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15eea29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  3,  6,  9, 12],\n",
       "       [ 2,  5,  8, 11, 14],\n",
       "       [ 1,  4,  7, 10, 13]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P @ C.T # Swap second and third rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11914971",
   "metadata": {},
   "source": [
    "- Imagine you have learned (or just made up) some coefficients from a classification or regression model that you want to apply to a matrix of (samples, features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "205b0da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35698f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.],\n",
       "       [ 3.,  4.,  5.],\n",
       "       [ 6.,  7.,  8.],\n",
       "       [ 9., 10., 11.],\n",
       "       [12., 13., 14.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C @ np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a783c07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28, 20,  4, 15, 24,  1, 14,  9, 18, 12],\n",
       "       [24,  2, 11,  5,  5, 19, 23,  0, 14,  4],\n",
       "       [24, 21, 18, 25,  4, 22, 17, 28,  5,  3]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57bfc0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.25],\n",
       "       [11.15],\n",
       "       [17.45]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = np.array([\n",
    "    [.1, .1, .3, 0.0, .2, .1, .1, .1, 0.0, 0.0]\n",
    "])\n",
    "\n",
    "bias = .05\n",
    "\n",
    "predictions = fake_counts @ coefficients.T + bias\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a77f6e",
   "metadata": {},
   "source": [
    "- Or, what if you have *many* regression/classification models that you want to apply to your samples at the same time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4271778d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients2d = np.random.randn(5, fake_counts.shape[1])\n",
    "\n",
    "bias1d = np.random.randn(1, 5)\n",
    "\n",
    "coefficients2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "331b1513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (3, 10)\n",
      "Output shape:  (3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.40187170e+01,  2.76790507e+01,  8.45469068e+00,\n",
       "         4.48748455e+01, -8.68258241e+00],\n",
       "       [-1.97325653e+01, -1.28759558e+01, -3.57315127e-02,\n",
       "        -8.99755510e+00, -4.87305601e+00],\n",
       "       [-8.76838795e+01,  1.42565908e+01,  5.90181244e+00,\n",
       "        -4.07572262e+01,  5.59406733e+00]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_2d = fake_counts @ coefficients2d.T + bias1d\n",
    "print(\"Input shape: \", fake_counts.shape)\n",
    "print(\"Output shape: \", pred_2d.shape)\n",
    "pred_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a39e2",
   "metadata": {},
   "source": [
    "Think about what's going on above. What do we have in the five columns of `pred_2d`? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc722d0a",
   "metadata": {},
   "source": [
    "This is what is going on in a \"fully connected\" neural network: we have a weight matrix (`pred_2d` here) that we multiply with some input. It is \"fully connected\" because each feature in our input is \"connected\" to each feature in our output (row-wise in the faux-example above). This is like running a bunch of different linear regression models at the same time. More on this soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f58a6b",
   "metadata": {},
   "source": [
    "## Introducing `torch`\n",
    "\n",
    "PyTorch is a machine learning library that is extremely powerful. Because most people who use `torch` are alerady familiar with `numpy`, `torch` implements many of the same methods. \n",
    "\n",
    "The `torch.tensor` is directly analogous to the `np.array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9de144c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce4dfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(torch.tensor(np.arange(10)), torch.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5539d",
   "metadata": {},
   "source": [
    "`Tensor`s, like numpy `arrays`, have a `shape` attribute, but this is just an alias for `size()`, which can be confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56c2b361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39cac161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a65f19",
   "metadata": {},
   "source": [
    "This looks effectively to how it would in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3eb42e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0625, 0.1250, 0.1875, 0.2500],\n",
       "        [0.0000, 0.1250, 0.2500, 0.3750, 0.5000],\n",
       "        [0.0000, 0.1875, 0.3750, 0.5625, 0.7500],\n",
       "        [0.0000, 0.2500, 0.5000, 0.7500, 1.0000]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space1d = torch.linspace(0, 1, 5)\n",
    "space2d = space1d[:, None] @ space1d[None, :]\n",
    "space2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358639b",
   "metadata": {},
   "source": [
    "As in Numpy, you can get a boolean array like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0edc7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space2d > .2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8a1a1",
   "metadata": {},
   "source": [
    "So if we want to set all elements below some threshold to 0.0, we can just do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe3b0a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.2500, 0.5625, 0.5625, 0.6250],\n",
       "        [0.0000, 0.5000, 1.1250, 1.1250, 1.2500],\n",
       "        [0.0000, 0.7500, 1.6875, 1.6875, 1.8750],\n",
       "        [0.0000, 1.0000, 2.2500, 2.2500, 2.5000]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (space2d > .2).type(torch.float32)\n",
    "space2d.T @ mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24264cb4",
   "metadata": {},
   "source": [
    "There are two key attributes of a `Tensor` that are not shared with numpy `array`s:\n",
    "- `requires_grad` records operations on the tensor for autograd. We will briefly discuss this when we go over neural networks. \n",
    "- `device` indicates which device on your machine the tensor is stored on. \n",
    "    - If you want to use your CPU, it should be `\"cpu\"`. \n",
    "    - If you have a Mac with Apple Silicon, you can specify `\"mps\"`. \n",
    "    - If you have an Nvidia graphics card, you can use `\"cuda\"`. \n",
    "\n",
    "To check if you have a device besides `\"cpu\"`, you can run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a9adede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfb45603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa8ac2",
   "metadata": {},
   "source": [
    "To move a `Tensor` (or other PyTorch object) from one device to another, you can use the `to()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16abb887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.type(torch.float32).to(\"cuda\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2acfafd",
   "metadata": {},
   "source": [
    "When performing any computation with multiple tensors, they all must be on the same device. This takes a while to get used to, and is a common source of errors when starting out with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fdab8a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors aren't on the same device!\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor(b, dtype=torch.float32).to(\"cpu\")\n",
    "\n",
    "try:\n",
    "    b = a[:, None] @ b[None, :]\n",
    "except RuntimeError:\n",
    "    print(\"Tensors aren't on the same device!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ce2a9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
       "        [0., 2., 0., 2., 0., 2., 0., 2., 0., 2.],\n",
       "        [0., 3., 0., 3., 0., 3., 0., 3., 0., 3.],\n",
       "        [0., 4., 0., 4., 0., 4., 0., 4., 0., 4.],\n",
       "        [0., 5., 0., 5., 0., 5., 0., 5., 0., 5.],\n",
       "        [0., 6., 0., 6., 0., 6., 0., 6., 0., 6.],\n",
       "        [0., 7., 0., 7., 0., 7., 0., 7., 0., 7.],\n",
       "        [0., 8., 0., 8., 0., 8., 0., 8., 0., 8.],\n",
       "        [0., 9., 0., 9., 0., 9., 0., 9., 0., 9.]], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.to(\"cuda\")\n",
    "\n",
    "a[:, None] @ b[None, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
