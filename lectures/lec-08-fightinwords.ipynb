{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab3fe9d-a990-4ea8-acba-cb55abc07673",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 08: Fightin' words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca529e-6ba2-460c-be50-082ab5951124",
   "metadata": {},
   "source": [
    "## Measuring distinctive words between corpora\n",
    "\n",
    "We often want to know which words are used differently in two corpora. There are a bunch of ways to do this. We can train classifiers and examine their feature weights. We can look at mutual information metrics. We could just count the words and see which ones are used more frequently in one corpus than another.\n",
    "\n",
    "## Fightin' words\n",
    "\n",
    "But a simple go-to approach that is robust to different underlying word frequencies and makes Bayesian assumptions about how often we would *expect* to see each word, given its frequency in a reference corpus, is Monroe et al.'s [Fightin' words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf).\n",
    "\n",
    "Using code originally developed by Jack Hessel (a Cornell PhD grad!), we've provided you with `fightinwords.py`. We'll walk through that code and then see how it performs on real-world data.\n",
    "\n",
    "The basic algorithm is to measure the observed frequency of each word in two corpora, (optionally) compare that frequency to an empirical prior, and normalize the result using a z-score. The words that have the largest magnitude z-scores (positive or negative) are the ones that tell us the most about the unique vocabulary of each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb39f1db-a25a-438b-bc90-d5f6e47745c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fightinwords as fw\n",
    "import numpy as np\n",
    "import os\n",
    "from   sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8b1b2-6f7f-4b81-b88d-cc1ac17e7f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test on two novels with 40 more for informative priors\n",
    "\n",
    "vectorizer = CountVectorizer( # set up a vectorizer\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "# get file names\n",
    "data_dir = os.path.join('..', 'data', 'texts')\n",
    "files = os.listdir(data_dir)\n",
    "bambi_file = 'O-Salten-Bambi-1923.txt'\n",
    "mme_bovary_file = 'F-Flaubert-Madame_Bovary-1857-M.txt'\n",
    "if '.DS_Store' in files: files.remove('.DS_Store') # remove MacOS cruft\n",
    "files.remove(bambi_file)\n",
    "files.remove(mme_bovary_file)\n",
    "corpus = [os.path.join(data_dir, file) for file in files] # background corpus\n",
    "samples = [os.path.join(data_dir, file) for file in [bambi_file, mme_bovary_file]]\n",
    "\n",
    "# read target files by line\n",
    "bambi_text = [fw.basic_sanitize(line) for line in open(samples[0], 'rt').readlines()]\n",
    "mme_bovary_text = [fw.basic_sanitize(line) for line in open(samples[1], 'rt').readlines()]\n",
    "\n",
    "# convenience function to FW display output\n",
    "def display_fw(data, n=10, name1='corpus one', name2='corpus two'):\n",
    "    '''Display the indicated number of top terms from fightinwords output.'''\n",
    "    print(\"Top terms in\", name1)\n",
    "    for term, score in reversed(data[-n:]):\n",
    "        print(f\"{term:<10} {score:6.3f}\")\n",
    "    print(\"\")\n",
    "    print(\"Top terms in\", name2)\n",
    "    for term, score in data[:n]:\n",
    "        print(f\"{term:<10} {score:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355233bb-4327-4c1a-9158-0b3ffd99ed1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results with a flat (noninformative) prior\n",
    "# note idiom: pass in text, use default vectorizer\n",
    "flat = fw.bayes_compare_language(bambi_text, mme_bovary_text)\n",
    "display_fw(flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ea480-29ac-44f2-a4ca-c0844c85bf9d",
   "metadata": {},
   "source": [
    "Meh. Not *wrong*, but I can't make much of this. Let's try it with an informative prior ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86d954-6e48-4af7-a299-aba716ccc72b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# learn vocab from *corpus* (not samples) and calculate priors\n",
    "priors = np.sum(vectorizer.fit_transform(corpus), axis=0).reshape(-1,1)\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748518c-f49c-477a-91a7-0d05024fbee0",
   "metadata": {},
   "source": [
    "Note that we have changed the input features in this case. We have learned word frequencies on a corpus the DOES NOT include either _Bambi_ or _Mme Bovary_. So we probably won't see \"Bambi\" as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1806f3-98ad-4899-9866-6334945f0725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorize test books using fitted vectorizer\n",
    "test_books = vectorizer.transform(samples) # NOT .fit(); want to keep existing vocabulary from priors\n",
    "print(test_books.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae9606-0c91-4a14-990e-9f02dcbf3103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use informative prior\n",
    "# different idiom: pass in index positions in pre-computed feature matrix\n",
    "informative = fw.bayes_compare_language(\n",
    "    l1=[0], \n",
    "    l2=[1], \n",
    "    features=test_books, \n",
    "    cv=vectorizer, \n",
    "    prior=priors, \n",
    "    #prior_weight=10 <- optional normalize and reweight the data\n",
    ")\n",
    "display_fw(informative, name1='Bambi', name2='Mme Bovary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea048ae-1ab4-4688-a65b-4950367ff1da",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952685ce-b047-4ed7-b05e-bf0750dc9b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from   adjustText import adjust_text # pretty, but not included with class environment\n",
    "                                     # conda install -c conda-forge adjusttext \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# convert to dataframe for convenience\n",
    "df = pd.DataFrame(test_books.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "num_words_to_plot = 20 # number of most distinctive words to plot from each corpus\n",
    "\n",
    " # manage data for plotting\n",
    "frequencies = []\n",
    "zscores = []\n",
    "words = []\n",
    "for word, z_score in informative:\n",
    "    count = df[word].sum()\n",
    "    if count > 0:\n",
    "        zscores.append(z_score)\n",
    "        words.append(word)\n",
    "        frequencies.append(count)\n",
    "\n",
    "# plot result\n",
    "texts = []\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(\n",
    "    np.log10(frequencies[:num_words_to_plot]), \n",
    "    zscores[:num_words_to_plot], \n",
    "    alpha=0.8, \n",
    "    label=\"Mme Bovary\"\n",
    ")\n",
    "ax.scatter(\n",
    "    np.log10(frequencies[-num_words_to_plot:]), \n",
    "    zscores[-num_words_to_plot:], \n",
    "    alpha=0.8, \n",
    "    label=\"Bambi\"\n",
    ")\n",
    "for i in range(-num_words_to_plot, num_words_to_plot):\n",
    "    texts.append(ax.text(np.log10(frequencies[i]), zscores[i], words[i], size='small', alpha=0.6))\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.5))\n",
    "plt.xlabel('log(frequency)')\n",
    "plt.ylabel('z-score')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11166c3d-2dca-40db-9821-c299338bec91",
   "metadata": {},
   "source": [
    "## News data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bc036-fd57-4dfd-ab69-b0f9a3ba2729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data from disk and examine\n",
    "import re\n",
    "\n",
    "news = pd.read_csv(os.path.join('..', 'data', 'news', 'news_text.csv.gz'))\n",
    "\n",
    "# a function to get rid of datelines at the start of articles\n",
    "#  matches one or more hyphens or colons in first 40 chars,\n",
    "#  drops everything before that match (plus the match itself)\n",
    "pattern = '[-:]+ '\n",
    "matcher = re.compile(pattern) # compiled regexs are faster\n",
    "\n",
    "def remove_dateline(text, matcher=matcher):\n",
    "    '''\n",
    "    Remove source names and datelines from a text string\n",
    "    If there is a hyphen or colon in the first 40 characters, \n",
    "      drops everything before the hyphen(s)/colon(s)\n",
    "    If no hyphen/colon, do nothing\n",
    "    Return processed string\n",
    "    '''\n",
    "    result = matcher.search(text, endpos=40)\n",
    "    if result:\n",
    "        return text[result.end():]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# clean article text\n",
    "news['body'] = news['body'].apply(remove_dateline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1999d3-e74a-4633-9495-b657637363f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_holdout_articles = 20000\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    input='content',\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "# calculate priors on non-holdout volumes\n",
    "priors = np.sum(vec.fit_transform(news.body.iloc[num_holdout_articles:]), axis=0).reshape(-1,1)\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a0c24-fd5f-4a1a-9c2b-f8ef70a6295b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sports = news.iloc[:num_holdout_articles].loc[news.label=='Sports', ['body']]\n",
    "other = news.iloc[:num_holdout_articles].loc[~(news.label=='Sports'), ['body']]\n",
    "result = fw.bayes_compare_language(\n",
    "    l1=[j for i, j in sports.itertuples()], \n",
    "    l2=[j for i, j in other.itertuples()], \n",
    "    cv=vec, \n",
    "    prior=priors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cb1fc-c5d5-4ffb-891a-9c425709d264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_fw(result, n=10, name1='sports', name2='other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e858c-6328-4d54-8a13-c545be6a6011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Words in prior: {np.sum(priors):>10}\")\n",
    "print(f\"Words in samples: {np.sum(vec.transform(news.body.iloc[:num_holdout_articles])):>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e10cb-62f6-400c-887d-4259b3710829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what's up with '39'?\n",
    "with pd.option_context(\"display.max_colwidth\", 150):\n",
    "    display(sports.loc[sports.body.str.contains('39')].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48c6e2-0053-4c9d-9ee9-b6a52eabbcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Fraction sports with '39':\", len(sports.loc[sports.body.str.contains('39')])/len(sports))\n",
    "print(\"Fraction other  with '39':\", len(other.loc[other.body.str.contains('39')])/len(other))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0e61c-d4bd-4502-b74e-8aef817fd605",
   "metadata": {},
   "source": [
    "Oh, well that's dumb. Clearly need more/different preprocessing. This is the sort of thing you want to check when you see odd results. That said, I guess sports articles use more apostrophes than do other articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4e3be-e2e0-44e4-91a6-d65a1f8eb39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compare without priors\n",
    "no_priors = fw.bayes_compare_language(\n",
    "    l1=[j for i, j in sports.itertuples()], \n",
    "    l2=[j for i, j in other.itertuples()], \n",
    "    cv=vec, \n",
    ")\n",
    "display_fw(no_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571412f-372d-46ed-8510-131d47d4a9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
