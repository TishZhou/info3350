{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 02: Vectorization\n",
    "\n",
    "* **Go to section, or you will fail the course.**\n",
    "* **Do the readings for the day that they are assigned, and at latest by Friday.**\n",
    "* **Watch/star the course repo so you don't miss any updates or schedule changes**\n",
    "* **First homework is released next Friday**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What did we go over last Wednesday?\n",
    "\n",
    "* Turning documents into vectors\n",
    "* Comparing vectors with distance/similarity metrics\n",
    "* Tokenization (split up strings into smaller units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TF-IDF weighting for word counts\n",
    "\n",
    "* Why do we sometimes remove stopwords from our features?\n",
    "    * High-frequency words shared by many documents don't tell us (in many, but not all, cases) much about the similarities or differences between documents\n",
    "* But stopword lists are binary: a word is either a stopword (hence, removed) or it isn't\n",
    "* Can we define a continuous adjustment for \"stoppiness\" that we apply to *every* word, depending on how widely used it is?\n",
    "* One approach is \"term frequency-inverse document frequency\" (TFIDF) weighting. \n",
    "    * You can think of this as multiplying the count of each term in a document by the inverse of the fraction of all documents in which that word occurs (hence \"term frequency [multiplied by] inverse document frequency\"). It's a bit more complicated than that (see below), but that's the idea. This upweights words that occur in relatively few documents.\n",
    "    * The count of a word that occurs in every document would be multiplied by one, hence get no boost in each document. A word that occurs in just one document in a corpus of 100 documents would be multiplied by 100 in the one document that contains it.\n",
    "* There are several tweaks to TFIDF to smooth it out and to modulate the boost it provides. `scikit-learn`'s `TfidfVectorizer` applies the reweighting:\n",
    "\n",
    "$$\\text{idf}(t) = \\ln\\frac{1+n}{1 + \\text{df}(t)} + 1$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $t$ is the term in question\n",
    "* $\\text{idf}(t)$ is the inverse document weight to be applied to the count of term $t$\n",
    "* $n$ is the number of documents in the corpus\n",
    "* $\\text{df}(t)$ is the number of documents in the corpus that contain term $t$\n",
    "\n",
    "A toy example: Consider two documents:\n",
    "\n",
    "* Document 1: `\"cat dog\"`\n",
    "* Document 2: `\"dog dog\"`\n",
    "\n",
    "`cat` occurs in just one document; `dog` occurs in both documents. So we want (and expect) to upweight the count of `cat` in document 1.\n",
    "\n",
    "Calculate the `idf` weight for `cat` in document 1:\n",
    "\n",
    "* $n = 2$\n",
    "* $\\text{df}(\\text{`cat'}) = 1$\n",
    "\n",
    "$$\\text{idf}(\\text{`cat'}) = \\ln\\frac{1 + 2}{1 + 1} + 1 = \\ln\\frac{3}{2} + 1 = 1.405$$\n",
    "\n",
    "And for `dog` in document 1:\n",
    "\n",
    "* $n = 2$\n",
    "* $\\text{df}(\\text{`dog'}) = 2$\n",
    "\n",
    "$$\\text{idf}(\\text{`dog'}) = \\ln\\frac{1 + 2}{1 + 2} + 1 = \\ln\\frac{3}{3} + 1 = 1.0$$\n",
    "\n",
    "So, `cat` will be upweighted relative to `dog`, because it is the less widely used word across documents in the corpus.\n",
    "\n",
    "Our non-normalized but IDF-weighted feature matrix will look like this:\n",
    "\n",
    "```\n",
    "cat  dog\n",
    "1.4  1.0\n",
    "0    2.0\n",
    "```\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corpus ###\n",
      " ['cat dog', 'dog dog'] \n",
      "\n",
      "### Feature matrix *without* IDF weighting ###\n",
      "Feature names: ['cat' 'dog']\n",
      "[[0.70710678 0.70710678]\n",
      " [0.         1.        ]]\n",
      "\n",
      "### Feature matrix *with* IDF weighting ###\n",
      "Feature names: ['cat' 'dog']\n",
      "[[0.81480247 0.57973867]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"cat dog\",\n",
    "    \"dog dog\",\n",
    "]\n",
    "print(\"### Corpus ###\\n\", corpus, \"\\n\")\n",
    "\n",
    "# without IDF weighting (note l2 norm)\n",
    "vectorizer_no_idf = TfidfVectorizer(\n",
    "    use_idf=False\n",
    ")\n",
    "features_no_idf = vectorizer_no_idf.fit_transform(corpus)\n",
    "print(\"### Feature matrix *without* IDF weighting ###\")\n",
    "print(\"Feature names:\", vectorizer_no_idf.get_feature_names_out())\n",
    "print(features_no_idf.toarray())\n",
    "\n",
    "# with IDF weighting\n",
    "vectorizer_with_idf = TfidfVectorizer(\n",
    "    use_idf=True,\n",
    ")\n",
    "features_with_idf = vectorizer_with_idf.fit_transform(corpus)\n",
    "print(\"\\n### Feature matrix *with* IDF weighting ###\")\n",
    "print(\"Feature names:\", vectorizer_with_idf.get_feature_names_out())\n",
    "print(features_with_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, in document 1, `cat` has been up-weighted while `dog` has been downweighted. There's no change in document 2 because that document has only a single word type and `TfidfVectorizer`'s `l2` norm enforces total feature weights whose squares sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2-normed, hand calculated, IDF weighted features for document 1\n",
      "[0.81471182 0.57986606]\n"
     ]
    }
   ],
   "source": [
    "# Check our hand calculation against code version\n",
    "import numpy as np\n",
    "vec = np.array([1.405, 1.0])     # hand calculation\n",
    "l2_vec = vec/np.linalg.norm(vec) # calculate l2 normalized version\n",
    "print(\"l2-normed, hand calculated, IDF weighted features for document 1\")\n",
    "print(l2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do our two versions match?\n",
    "assert np.allclose(l2_vec, features_with_idf[0,].toarray(), atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom functions\n",
    "\n",
    "`Sklearn` vectorizers have [settings for common options](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). But sometimes, you need to plug in your own code for a special case. For example, what about Chinese-language input? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['因为无法解决与中国合作伙伴的纠纷' '因受新型冠状病毒危机对足球和其他体育赛事的持续影响' '已终止了其最赚钱的海外转播合同'\n",
      " '早已面临越来越多亏损的英格兰超级足球联赛周四宣布']\n",
      "Values: [[0.5 0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Strings\n",
    "zh = '因受新型冠状病毒危机对足球和其他体育赛事的持续影响，早已面临越来越多亏损的英格兰超级足球联赛周四宣布，因为无法解决与中国合作伙伴的纠纷，已终止了其最赚钱的海外转播合同。'\n",
    "en = 'The English Premier League, already facing mounting losses because of the continued impact of the coronavirus crisis on soccer and other sporting events, announced on Thursday that it had canceled its most lucrative overseas broadcast contract after it was unable to resolve a dispute with its Chinese partner.'\n",
    "\n",
    "vectorizer_default = TfidfVectorizer(input='content')\n",
    "chinese_features = vectorizer_default.fit_transform([zh])\n",
    "print(\"Feature names:\", vectorizer_default.get_feature_names_out())\n",
    "print(\"Values:\", chinese_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/Teaching/info3350-f25/.venv/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.953 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['。' '与' '中国' '了' '亏损' '体育赛事' '其' '其他' '冠状病毒' '危机' '合作伙伴' '合同' '周四' '和'\n",
      " '因为' '因受' '多' '宣布' '对' '已' '影响' '持续' '新型' '无法' '早已' '最' '海外' '的' '纠纷'\n",
      " '终止' '英格兰' '解决' '赚钱' '超级' '越来越' '足球' '足球联赛' '转播' '面临' '，']\n",
      "Values: [[0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.50395263 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "# Custom tokenizer\n",
    "import jieba\n",
    "\n",
    "def chinese_tokenizer(x):\n",
    "    '''Tokenize a Chinese-language string'''\n",
    "    return jieba.lcut(x)\n",
    "    \n",
    "vectorizer_chinese = TfidfVectorizer(\n",
    "    input='content',\n",
    "    tokenizer=chinese_tokenizer,\n",
    "    token_pattern=None\n",
    ")\n",
    "chinese_features = vectorizer_chinese.fit_transform([zh])\n",
    "print(\"Feature names:\", vectorizer_chinese.get_feature_names_out())\n",
    "print(\"Values:\", chinese_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese vector length: 1.0\n"
     ]
    }
   ],
   "source": [
    "# check norm = l2\n",
    "# cf. https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
    "\n",
    "print(\"Chinese vector length:\", np.linalg.norm(chinese_features.toarray().T, ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should take a look at all of the other parameters for `sklearn` implementation of TF-IDF. The [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) is very clear and includes examples similar to the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "* What are some obvious limitations of TF-IDF? \n",
    "    * Say you fit a TF-IDF model to a large number of documents. What would you expect the resulting vectors to look like? What properties would they have?\n",
    "    * Say you add some new documents to your corpus after already fitting a TF-IDF model to it. Can you update it?\n",
    "    * How might you compare two or more TF-IDF vectors? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
